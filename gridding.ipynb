{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d248020-1c6a-4001-8360-a7dff6354a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7628cac-d383-4eff-8c79-5ea119200c6b",
   "metadata": {},
   "source": [
    "# Gridding\n",
    "\n",
    "Here we present multiple solutions and approaches on the gridding problem for a potentially huge dataset.\n",
    "\n",
    "## Table of Content\n",
    "0. [Summary](#Summary)\n",
    "1. [Python](#1.-python/)\n",
    "   - [1.1 Benchmarks](#1.1-Benchmarks)\n",
    "     - [1.1.1 Original](#1.1.1-Original)\n",
    "     - [1.1.2 Versions 1 to 5](#1.1.2-Versions-1-to-5)\n",
    "     - [1.1.3 Version 6 - Multithreaded](#1.1.3-Version-6---Multithreaded)\n",
    "     - [1.1.4 Versions 7/8 - MPI Distributed](#1.1.4-Versions-7/8-MPI-Distributed)\n",
    "2. [C Python](#2.C_python/)\n",
    "   - [2.1 Benchmarks](#2.1-Benchmarks)\n",
    "     - [2.1.1 Version 1 - OpenMP Threaded](#2.1.1-Version-1---OpenMP-Threaded)\n",
    "     - [2.1.2 Version 2 - MPI Distributed](#2.1.2-Version-2---MPI-Distributed)\n",
    "     - [2.1.3 Version 3 - SIMD/OpenMP](#2.1.3-Version-3---SIMD/OpenMP)\n",
    "     - [2.1.4 Version 4 - SIMD/MPI/OpenMP](#2.1.4-Version-4---SIMD/MPI/OpenMP)\n",
    "3. [CUDA Python](#3.CUDA_python/)\n",
    "   - [3.1 Benchmarks](#3.1-Benchmarks)\n",
    "     - [3.1.1 Version 1 - Single GPU](#3.1.1-Version-1---Single-GPU)\n",
    "     - [3.1.2 Version 2 - Multi-GPU](#3.1.2-Version-2---Multi-GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d672d4-a7f0-48bb-9aa0-edcc2017983a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- We perform gridding using an optimized and vectorized versions in python, achieve 100 times speedup by vectorization and Just In Time (JIT) compiling with loading the data in whole into memory, also achieve 100 times speedup using python threads, but with chunked data access.\n",
    "- Scale the python code to 4 threads and almost linearly to 18 MPI processes, until the communication and thread overhead becomes overwhelming for the data size.\n",
    "- We offload the gridding function to C, and implement different versions for OpenMP, MPI and SIMD version of the gridding.\n",
    "- With OpenMP threads we scale the gridding to 32 threads (after which thread overhead takes over), and gain over 6000 times speedup over the original serial version of the code.\n",
    "- With MPI we scale upto 8 processes after which communication time becomes a bottleneck. We achieve a 600 times speedup over the original serial version of the code.\n",
    "- Furthermore we manullay vectorize the gridding function using SIMD instruction set which results in almost 60% better performance when using OpenMP version alone.\n",
    "- We offload the gridding to multiple GPUs using CUDA and MPI. The gridding kernel takes on the order of milliseconds and scales linearly with the number of GPUs. But the allocation of the data on GPUs and the data movement between CPU and GPU takes 3 orders of magnitude more, which becomes the bottleneck.\n",
    "\n",
    "In what follows we put the detailed plots of the different approaches and benchmarking results of the different versions: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26553d83-79f0-4974-b6af-4ca22769bd9f",
   "metadata": {},
   "source": [
    "## 1. `python/` \n",
    "\n",
    "Contains the treatment of the problem in pure python, the approaches taken on each of the versions are as follow:\n",
    "\n",
    "   - `v0_original.py`: The original version of the code.\n",
    "   - `v1_index_calc_jitted.py`: Calculating the grid indices has been Just In Compiled (JIT).\n",
    "   - `v2_gridding_jitted.py`: The whole gridding function has been compiled.\n",
    "   - `v3_single_timestep_vectorized.py`: Calculation of a single timestep of the grid has been fully vectorized using numpy intrinsic functions.\n",
    "   - `v4_single_timestep_vectorized_jitted.py`: On top of vectorizing a single timestep the function for calculating a single timestep of the grid has been compiled.\n",
    "   - `v5_gridding_vectorized.py`: The whole gridding function has been fully vectorized using numpy.\n",
    "   - `v6_gridding_vectorized_multithreaded.py`: On top of vectorizing the gridding function it uses python `concurrent` library to parallelize the gridding over `n_workers` threads using chunks of the dataset.\n",
    "   - `v7_mpi_timesteps.py`: Using `mpi4py` library we divide the computation of the grid over timsteps to multiple processes.\n",
    "   - `v8_mpi_baselines.py`: Same as above but divide the dataset over baseline pairs wrather than timesteps to multiple process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23110084-cdca-4e8b-8afb-fb8edb2d034e",
   "metadata": {},
   "source": [
    "### 1.1 Benchmarks\n",
    "\n",
    "Here we present the benchmarking results obtained on pure python implementations. Note that these benchmarks are done on a single node, with *dual sockets* and an *AMD EPYC 7H12 64-Core Processor* per socket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26270b48-eb09-4ec5-b07e-ebc10dbff9bd",
   "metadata": {},
   "source": [
    "#### 1.1.1 Original\n",
    "\n",
    "<center><img src=\"python/plots/v0_original.png\" alt=\"v0\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2283fc8b-4da3-4b20-9c03-293807518459",
   "metadata": {},
   "source": [
    "From the benchmark above we see that the most time consuming and the bottleneck is the `gridding` function as we expected from the 3 nestes loops in python! For this reason we will focus our attention on the gridding function and will present the different results and strategies on that.\n",
    "\n",
    "Here we put the benchamrking for the different version of the code, without any explicit parallelism by us (e.g. the benchmarks below are for the versions 1 to 5 without any multi-threading or use of MPI multi-processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119c2bf-1580-4570-acee-70f6af7bfbcf",
   "metadata": {},
   "source": [
    "#### 1.1.2 Versions 1 to 5\n",
    "\n",
    "| Version  | Time(s) |\n",
    "| ------------------ | --------|\n",
    "| `v1_index_calc_jitted`                 | 66.40 |\n",
    "| `v2_gridding_jitted`                   | 1.75  |\n",
    "| `v3_single_timestep_vectorized`        | 23.63 |\n",
    "| `v4_single_timestep_vectorized_jitted` | 26.77 |\n",
    "| `v5_gridding_vectorized`               | 2.09  |\n",
    "\n",
    "<center><img src=\"python/plots/v1tov5.png\" alt=\"v1tov5\"/></center>\n",
    "\n",
    "Here we see that by Just In Time (JIT) compiling the index calculation we achieve **3 times** better speed, further more by vectorizing a single timestep of the grid calculation we gain another speedup, but the real speedup comes from the fact that we JIT the whole gridding function and gain **100 times** speedup, **but** this comes at a cost, we have to **convert the data into numpy arrays** and hence move the data into memory at once before we are able to JIT the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80724f33-703a-4c57-a3d1-8caa0fd7b993",
   "metadata": {},
   "source": [
    "#### 1.1.3 Version 6 - Multithreaded\n",
    "\n",
    "| Version 6 (Multithreaded): Number of Threads  | Time(s) |\n",
    "| ------------------ | --------|\n",
    "| 1   | 2.16 |\n",
    "| 2   | 1.68 |\n",
    "| 4   | 1.61 |\n",
    "| 8   | 1.99 |\n",
    "| 16  | 3.34 |\n",
    "\n",
    "<center><img src=\"python/plots/v6.png\" alt=\"v6\"/></center>\n",
    "\n",
    "By multithreading the access to the data we don't have load the data at whole into memory and **we can do it chunks**, this way we don't achieve the same speedups, but compared to single threaded original version with gain almost the same **100 times** speedup with 4 threads. We note that as we increase the number of threads we observe the overhead of maintaining those threads overwhelems the gains and thus we start to increase in total time of the gridding after 4 threads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c90bd9c-d7a2-45d0-97ea-db8244981027",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.1.4 Versions 7/8 MPI Distributed\n",
    "\n",
    "\n",
    "| Version 7/8 (MPI): Number of MPI Processes  | `gridding_timesteps` Time(s)  | `gridding_baselines`  Time(s) |\n",
    "| ------------------ | --------| ------- |\n",
    "| 1  | 94.56 | 34.29 | \n",
    "| 2  | 18.37 | 24.25 |\n",
    "| 4  | 6.28  | 11.65 |\n",
    "| 8  | 3.84  | 9.02  |\n",
    "| 16 | 3.61  | 11.38 |\n",
    "\n",
    "<center><img src=\"python/plots/v7v8.png\" alt=\"v7v8\"/></center>\n",
    "\n",
    "Here we divide the calculation of grid over timesteps (v7) and baselines (v8) to multiple processes. Division over timestep apart from achieving a better scaling has the added advantage of **loading the data in chunks** wrather than loading the dataset in whole for the baseline version. We observe that multithreaded version as it is veectorized fully across all 3 dimensions performs almost **2 times** better than MPI version. More so the division of the grid over timesteps performs better and achieves better scaling than division on baselines because the data access patterns in memory is better for the first one and is aligned in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533fe2b-ee62-4d87-8852-679695910ded",
   "metadata": {},
   "source": [
    "## 2.`C_python/`\n",
    "\n",
    "Here we offload the gridding function to C and use the `ctypes` library to load the shared library. Different version of the sources are as follow:\n",
    "\n",
    "- `libgrid.c`: The library cotaining different implementations of the gridding function. To make the library an MPI compiler must be present, then issue `make` in the directory which uses GNU `mpicc` compiler with `-O3` optimization.\n",
    "- `v1_omp.py`: Uses `gridding_omp` function which parallelized the gridding with OpenMP threads.\n",
    "- `v2_mpi_omp.py`: Uses the hybrid MPI/OpenMP `grdding_mpi_omp` function, divides the grid in timesteps over MPI processes and parallelizes the loop with OpenMP threads.\n",
    "- `v3_simd.py`: Uses the `gridding_simd` function which implements a fully vectorized gridding manually, note that with `-O3` flag, compiler basically does that for us, but here we have done it manually also as an exercise.\n",
    "- `v4_simd_mpi_omp.py`: Uses the `grdding_simd_mpi_omp` function, which basically combines all the previous 3 implementations, divides the grid over timesteps to multiple processes, parallelizes the loop with OpenMP threads over a fully vectorized gridding implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f2bde-bf3b-4a9f-b179-51be03590786",
   "metadata": {},
   "source": [
    "### 2.1 Benchmarks\n",
    "\n",
    "The benchmarks here are also done on the same EPYC node. Note the significant speedup and scaling we achieve by offloading the gridding function to C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d391cb9b-dfd5-46d4-a3f5-1cc267eeb224",
   "metadata": {},
   "source": [
    "#### 2.1.1 Version 1 - OpenMP Threaded\n",
    "\n",
    "| Version 1 (OpenMP): Number of Threads  | Time(s) |\n",
    "| ------------------ | --------|\n",
    "| 1   | 0.316 |\n",
    "| 2   | 0.172 |\n",
    "| 4   | 0.094 |\n",
    "| 8   | 0.053 |\n",
    "| 16  | 0.035 |\n",
    "| 32  | 0.034 |\n",
    "| 64  | 0.037 |\n",
    "| 128 | 0.048 |\n",
    "\n",
    "<center><img src=\"C_python/plots/v1.png\" alt=\"v1\"/></center>\n",
    "\n",
    "Note that with the C OpenMP implementation we achieve **4 orders of magnitude** speedup compared to the original version. Note that the best scaling achieved upto 32 threads and after that synchronization between threads overwhelms the gain in threading and as we move toward more threads we acutally spent more time on threading overhead than gaining. Note also that compared to python threads we almost have a speedup of **100 times**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b14c2a-9894-4ff7-94fa-b3b1ed1d4e99",
   "metadata": {},
   "source": [
    "#### 2.1.2 Version 2 - MPI Distributed\n",
    "\n",
    "\n",
    "| Version 2 (MPI): Number of Processes  | Time(s) |\n",
    "| ------------------ | --------|\n",
    "| 1   | 0.340 |\n",
    "| 2   | 0.198 |\n",
    "| 4   | 0.181 |\n",
    "| 8   | 0.170 |\n",
    "| 16  | 0.198 |\n",
    "| 32  | 0.306 |\n",
    "| 64  | 0.386 |\n",
    "| 128 | 0.674 |\n",
    "\n",
    "<center><img src=\"C_python/plots/v2.png\" alt=\"v2\"/></center>\n",
    "\n",
    "Note that with MPI parallelization we achieve a scaling upto 8 processes and after that the **ccommunication time** between processes takes over the computation which is expected for a problem of this size, we actually need a large dataset to see the benifits of MPI distribution over multiple nodes. Note that here most of the time is spent in the `MPI_Reduce` call inside the library. Also an interesting fact is that because of this we almost perform **10 times** worse than OpenMP threads but **10 times** better than MPI launched directly with python.\n",
    "We observe that for the problem we have threads perform better than MPI processes because of excessive I/O for which threads perform better because they have a shared memory wrather than private memory of MPI processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced1e94-66e5-4080-af15-e71e1bac7f03",
   "metadata": {},
   "source": [
    "#### 2.1.3 Version 3 - SIMD/OpenMP\n",
    "\n",
    "\n",
    "| Version 3 (SIMD/OpenMP): Number of Threads  | Time(s) |\n",
    "| ------------------ | --------|\n",
    "| 1   | 0.162 |\n",
    "| 2   | 0.088 |\n",
    "| 4   | 0.045 |\n",
    "| 8   | 0.030 |\n",
    "| 16  | 0.026 |\n",
    "| 32  | 0.030 |\n",
    "| 64  | 0.034 |\n",
    "| 128 | 0.045 |\n",
    "\n",
    "<center><img src=\"C_python/plots/v3.png\" alt=\"v3\"/></center>\n",
    "\n",
    "Note that as we have compiled the code with `-O3` flag compiler can do vectorization as it can deduce. Here we have implemented a manual vectorization using SIMD intrinsics. We see almost **60% speedup** over a pure OpenMP implementation with 16 threads, which is also compiled with `-O3`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e5cb4-510c-4a8e-9b5e-a9f8cec549f8",
   "metadata": {},
   "source": [
    "#### 2.1.4 Version 4 - SIMD/MPI/OpenMP\n",
    "\n",
    "\n",
    "| Version 4 (SIMD/MPI/OpenMP): Number of Processes  | Time(s) |\n",
    "| ------------------ | --------|\n",
    "| 1   | 0.162 |\n",
    "| 2   | 0.088 |\n",
    "| 4   | 0.045 |\n",
    "| 8   | 0.030 |\n",
    "| 16  | 0.026 |\n",
    "| 32  | 0.030 |\n",
    "| 64  | 0.034 |\n",
    "| 128 | 0.045 |\n",
    "\n",
    "<center><img src=\"C_python/plots/v3.png\" alt=\"v3\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6932d84-03a8-442a-b0e4-46da0b1e7961",
   "metadata": {},
   "source": [
    "## 3.`CUDA_python/`\n",
    "\n",
    "Here we offload the gridding function to CUDA and use the `ctypes` library to load the shared library. Different version of the sources are as follow:\n",
    "\n",
    "- `libgrid.cu`: The library cotaining the single GPU implementation of the gridding function. To compile it issue `make` in the directory, for which a CUDA capable compiler is needed, which here we have uses `nvcc` from NVIDIA SDK.\n",
    "- `libgird_mpi.cu`: The library containing the multi-GPU implementation of the gridding function using CUDA and MPI. To compile it issue `make mpi` in the directory, for which linking against the MPI library is needed, note that here we don't assume a CUDA aware implementation, hence this forces us extra data movement when we are reducing the results from different GPUs.\n",
    "- `v1_cuda_.py`: Uses `libgrid` shared library to perform gridding on a single GPU.\n",
    "- `v2_cuda_mpi.py`: Uses `libgrid_mpi` shared library so perform gridding on multiple GPUs. The gridding over timsteps is divided between different GPUs using MPI processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea5e905-f02f-4f3c-814a-972b2f68e4f1",
   "metadata": {},
   "source": [
    "### 3.1 Benchmarks\n",
    "\n",
    "The GPU benchmarks are done a node with 2 NVIDIA V100 SXM2 32GB GPUs with an Intel Xeon Gold 6226 CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd52c4-a8a7-47cc-8fbf-deb308c76562",
   "metadata": {},
   "source": [
    "#### 3.1.1 Version 1 - Single GPU\n",
    "\n",
    "\n",
    "| Function  | Time(s) |\n",
    "| ------------------  | ------|\n",
    "| `gridding`          | 1.444 |\n",
    "| `gridding_kernel`   | 0.005 |\n",
    "\n",
    "<center><img src=\"CUDA_python/plots/v1.png\" alt=\"v1\"/></center>\n",
    "\n",
    "The actual gridding kernel which is done on GPU is done in the order of **milliseconds**, most of the time is spent on **data copy** between CPU and GPU, which is the bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a87c1-a68b-4537-b031-d663fb42eea8",
   "metadata": {},
   "source": [
    "#### 3.1.2 Version 2 - Multi-GPU\n",
    "\n",
    "\n",
    "| Version 2 (CUDA/MPI): Number of MPI Processes  | `gridding` Time(s)  | `gridding_kernel`  Time(s) |\n",
    "| ------------------ | --------| ------- |\n",
    "| 1   | 1.526 | 0.005 | \n",
    "| 2   | 1.608 | 0.003 |\n",
    "\n",
    "<center><img src=\"CUDA_python/plots/v2.png\" alt=\"v2\"/></center>\n",
    "\n",
    "We observe that the kernel scales almost lineatly to 2 GPUs, but then the whole gridding function because of data movement between GPUs and CPUs has remained almost the same, which prohibits the further scaling over GPUs. The problem size here for two GPUs is not that large and hence the **communication time and the data movement** part takes over the actual computation pretty quickly and by 3 orders of magnitude"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
