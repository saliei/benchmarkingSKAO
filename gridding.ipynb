{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d248020-1c6a-4001-8360-a7dff6354a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7628cac-d383-4eff-8c79-5ea119200c6b",
   "metadata": {},
   "source": [
    "# Gridding\n",
    "\n",
    "Here we present multiple solutions and approaches on the gridding problem for a potentially huge dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d672d4-a7f0-48bb-9aa0-edcc2017983a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- We achieve a **10 fold** increase...\n",
    "- scaling\n",
    "- C\n",
    "- CUDA\n",
    "\n",
    "In what follows we put the detailed study of the different approaches and benchmarking results for different versions: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26553d83-79f0-4974-b6af-4ca22769bd9f",
   "metadata": {},
   "source": [
    "## 1. `python/` \n",
    "\n",
    "Contains the treatment of the problem in pure python, the approaches taken on each of the versions are as follow:\n",
    "\n",
    "   - `v0_original.py`: The original version of the code.\n",
    "   - `v1_index_calc_jitted.py`: Calculating the grid indices has been Just In Compiled (JIT).\n",
    "   - `v2_gridding_jitted.py`: The whole gridding function has been compiled.\n",
    "   - `v3_single_timestep_vectorized.py`: Calculation of a single timestep of the grid has been fully vectorized using numpy intrinsic functions.\n",
    "   - `v4_single_timestep_vectorized_jitted.py`: On top of vectorizing a single timestep the function for calculating a single timestep of the grid has been compiled.\n",
    "   - `v5_gridding_vectorized.py`: The whole gridding function has been fully vectorized using numpy.\n",
    "   - `v6_gridding_vectorized_multithreaded.py`: On top of vectorizing the gridding function it uses python `concurrent` library to parallelize the gridding over `n_workers` threads using chunks of the dataset.\n",
    "   - `v7_mpi_timesteps.py`: Using `mpi4py` library we divide the computation of the grid over timsteps to multiple processes.\n",
    "   - `v8_mpi_baselines.py`: Same as above but divide the dataset over baseline pairs wrather than timesteps to multiple process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23110084-cdca-4e8b-8afb-fb8edb2d034e",
   "metadata": {},
   "source": [
    "### 1.1 Benchmarks\n",
    "\n",
    "Here we present the benchmarking results obtained on pure python implementations. Note that these benchmarks are done on a single node, with *dual sockets* and an *AMD EPYC 7H12 64-Core Processor* per socket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26270b48-eb09-4ec5-b07e-ebc10dbff9bd",
   "metadata": {},
   "source": [
    "#### 1.1.1 Original\n",
    "\n",
    "<center><img src=\"python/plots/v0_original.png\" alt=\"v0\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2283fc8b-4da3-4b20-9c03-293807518459",
   "metadata": {},
   "source": [
    "From the benchmark above we see that the most time consuming and the bottleneck is the `gridding` function as we expected from the 3 nestes loops in python! For this reason we will focus our attention on the gridding function and will present the different results and strategies on that.\n",
    "\n",
    "Here we put the benchamrking for the different version of the code, without any explicit parallelism by us (e.g. the benchmarks below are for the versions 1 to 5 without any multi-threading or use of MPI multi-processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119c2bf-1580-4570-acee-70f6af7bfbcf",
   "metadata": {},
   "source": [
    "#### 1.1.2 Versions 1 to 5\n",
    "\n",
    "<center><img src=\"python/plots/v1tov5.png\" alt=\"v1tov5\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80724f33-703a-4c57-a3d1-8caa0fd7b993",
   "metadata": {},
   "source": [
    "#### 1.1.3 Version 6 - Multithreaded\n",
    "\n",
    "<center><img src=\"python/plots/v6.png\" alt=\"v6\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c90bd9c-d7a2-45d0-97ea-db8244981027",
   "metadata": {},
   "source": [
    "\n",
    "#### 1.1.4 Versions 7/8 MPI distributed\n",
    "\n",
    "\n",
    "<center><img src=\"python/plots/v7v8.png\" alt=\"v7v8\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533fe2b-ee62-4d87-8852-679695910ded",
   "metadata": {},
   "source": [
    "## 2.`C_python/`\n",
    "\n",
    "Here we offload the gridding function to C and use the `ctypes` library to load the shared library. Different version of the sources are as follow:\n",
    "\n",
    "- `libgrid.c`: The library cotaining different implementations of the gridding function. To make the library an MPI compiler must be present, then issue `make` in the directory which uses GNU `mpicc` compiler with `-O3` optimization.\n",
    "- `v1_omp.py`: Uses `gridding_omp` function which parallelized the gridding with OpenMP threads.\n",
    "- `v2_mpi_omp.py`: Uses the hybrid MPI/OpenMP `grdding_mpi_omp` function, divides the grid in timesteps over MPI processes and parallelizes the loop with OpenMP threads.\n",
    "- `v3_simd.py`: Uses the `gridding_simd` function which implements a fully vectorized gridding manually, note that with `-O3` flag, compiler basically does that for us, but here we have done it manually also as an exercise.\n",
    "- `v4_simd_mpi_omp.py`: Uses the `grdding_simd_mpi_omp` function, which basically combines all the previous 3 implementations, divides the grid over timesteps to multiple processes, parallelizes the loop with OpenMP threads over a fully vectorized gridding implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f2bde-bf3b-4a9f-b179-51be03590786",
   "metadata": {},
   "source": [
    "### 2.1 Benchmarks\n",
    "\n",
    "The benchmarks here are also done on the same EPYC node. Note the significant speedup and scaling we achieve by offloading the gridding function to C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d391cb9b-dfd5-46d4-a3f5-1cc267eeb224",
   "metadata": {},
   "source": [
    "#### 2.1.1 Version 1 - OpenMP Threaded\n",
    "\n",
    "| Version 1 (OpenMP): Number of Threads  | Time(s) |\n",
    "| ------------------ | --------|\n",
    "| 1   | 0.316 |\n",
    "| 2   | 0.172 |\n",
    "| 4   | 0.094 |\n",
    "| 8   | 0.053 |\n",
    "| 16  | 0.035 |\n",
    "| 32  | 0.034 |\n",
    "| 64  | 0.037 |\n",
    "| 128 | 0.048 |\n",
    "\n",
    "<center><img src=\"C_python/plots/v1.png\" alt=\"v1\"/></center>\n",
    "\n",
    "Note that with the C OpenMP implementation we achieve **4 orders of magnitude** speedup compared to the original version. Note that the best scaling achieved upto 32 threads and after that synchronization between threads overwhelms the gain in threading and as we move toward more threads we acutally spent more time on threading overhead than gaining. Note also that compared to python threads we almost have a speedup of **100 times**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b14c2a-9894-4ff7-94fa-b3b1ed1d4e99",
   "metadata": {},
   "source": [
    "#### 2.1.2 Version 2 - MPI distributed\n",
    "\n",
    "\n",
    "| Version 2 (MPI): Number of Processes  | Time(s) |\n",
    "| ------------------ | --------|\n",
    "| 1   | 0.340 |\n",
    "| 2   | 0.198 |\n",
    "| 4   | 0.181 |\n",
    "| 8   | 0.170 |\n",
    "| 16  | 0.198 |\n",
    "| 32  | 0.306 |\n",
    "| 64  | 0.386 |\n",
    "| 128 | 0.674 |\n",
    "\n",
    "<center><img src=\"C_python/plots/v2.png\" alt=\"v2\"/></center>\n",
    "\n",
    "Note that with MPI parallelization we achieve a scaling upto 8 processes and after that the **ccommunication time** between processes takes over the computation which is expected for a problem of this size, we actually need a large dataset to see the benifits of MPI distribution over multiple nodes. Note that here most of the time is spent in the `MPI_Reduce` call inside the library. Also an interesting fact is that because of this we almost perform **10 times** worse than OpenMP threads but **10 times** better than MPI launched directly with python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced1e94-66e5-4080-af15-e71e1bac7f03",
   "metadata": {},
   "source": [
    "#### 2.1.3 Version 3 - SIMD/OpenMP\n",
    "\n",
    "\n",
    "| Version 3 (SIMD/OpenMP): Number of Threads  | Time(s) |\n",
    "| ------------------ | --------|\n",
    "| 1   | 0.162 |\n",
    "| 2   | 0.088 |\n",
    "| 4   | 0.045 |\n",
    "| 8   | 0.030 |\n",
    "| 16  | 0.026 |\n",
    "| 32  | 0.030 |\n",
    "| 64  | 0.034 |\n",
    "| 128 | 0.045 |\n",
    "\n",
    "<center><img src=\"C_python/plots/v3.png\" alt=\"v3\"/></center>\n",
    "\n",
    "Note that as we have compiled the code with `-O3` flag compiler can do vectorization as it can deduce. Here we have implemented a manual vectorization using SIMD intrinsics. We see almost **60% speedup** over a pure OpenMP implementation with 16 threads, which is also compiled with `-O3`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e5cb4-510c-4a8e-9b5e-a9f8cec549f8",
   "metadata": {},
   "source": [
    "#### 2.1.4 Version 4 - SIMD/MPI/OpenMP\n",
    "\n",
    "\n",
    "| Version 4 (SIMD/MPI/OpenMP): Number of Processes  | Time(s) |\n",
    "| ------------------ | --------|\n",
    "| 1   | 0.162 |\n",
    "| 2   | 0.088 |\n",
    "| 4   | 0.045 |\n",
    "| 8   | 0.030 |\n",
    "| 16  | 0.026 |\n",
    "| 32  | 0.030 |\n",
    "| 64  | 0.034 |\n",
    "| 128 | 0.045 |\n",
    "\n",
    "<center><img src=\"C_python/plots/v3.png\" alt=\"v3\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6932d84-03a8-442a-b0e4-46da0b1e7961",
   "metadata": {},
   "source": [
    "## 3.`CUDA_python/`\n",
    "\n",
    "Here we offload the gridding function to CUDA and use the `ctypes` library to load the shared library. Different version of the sources are as follow:\n",
    "\n",
    "- `libgrid.cu`: The library cotaining the single GPU implementation of the gridding function. To compile it issue `make` in the directory, for which a CUDA capable compiler is needed, which here we have uses `nvcc` from NVIDIA SDK.\n",
    "- `libgird_mpi.cu`: The library containing the multi-GPU implementation of the gridding function using CUDA and MPI. To compile it issue `make mpi` in the directory, for which linking against the MPI library is needed, note that here we don't assume a CUDA aware implementation, hence this forces us extra data movement when we are reducing the results from different GPUs.\n",
    "- `v1_cuda_.py`: Uses `libgrid` shared library to perform gridding on a single GPU.\n",
    "- `v2_cuda_mpi.py`: Uses `libgrid_mpi` shared library so perform gridding on multiple GPUs. The gridding over timsteps is divided between different GPUs using MPI processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea5e905-f02f-4f3c-814a-972b2f68e4f1",
   "metadata": {},
   "source": [
    "### 3.1 Benchmarks\n",
    "\n",
    "The GPU benchmarks are done a node with 2 NVIDIA V100 SXM2 32GB GPUs with an Intel Xeon Gold 6226 CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd52c4-a8a7-47cc-8fbf-deb308c76562",
   "metadata": {},
   "source": [
    "#### 3.1.1 Version 1 - Single GPU\n",
    "\n",
    "\n",
    "| Function  | Time(s) |\n",
    "| ------------------  | ------|\n",
    "| `gridding`          | 1.444 |\n",
    "| `gridding_kernel`   | 0.005 |\n",
    "\n",
    "<center><img src=\"CUDA_python/plots/v1.png\" alt=\"v1\"/></center>\n",
    "\n",
    "The actual gridding kernel which is done on GPU is done in the order or milliseconds, most of the time is spent on **data copy** between CPU and GPU, which is the bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a87c1-a68b-4537-b031-d663fb42eea8",
   "metadata": {},
   "source": [
    "#### 3.1.2 Version 2 - Multi-GPU\n",
    "\n",
    "\n",
    "| Version 2 (CUDA/MPI): Number of MPI Processes  | `gridding` Time(s)  | `gridding_kernel`  Time(s) |\n",
    "| ------------------ | --------| ------- |\n",
    "| 1   | 1.526 | 0.005 | \n",
    "| 2   | 1.608 | 0.003 |\n",
    "\n",
    "<center><img src=\"CUDA_python/plots/v2.png\" alt=\"v2\"/></center>\n",
    "\n",
    "We observe that the kernel scales almost lineatly to 2 GPUs, but then the whole gridding function because of data movement between GPUs and CPUs has remained almost the same, which prohibits the further scaling over GPUs. The problem size here for two GPUs is not that large and hence the **communication time and the data movement** part takes over the actual computation pretty quickly and by 3 orders of magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faad084-1b57-49ca-b5f8-8c3d16cb968f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
